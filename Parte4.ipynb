{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etapa 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta secção será realizado arquiteturas de deep learning como DNN e o CNN onde o objetivo será otmixar hiperparametros destas mesmas arquiteturas tendo em conta que o custo computacional é elevado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1=\"X_train.csv\"\n",
    "path2=\"y_train.csv\"\n",
    "path3=\"X_val.csv\"\n",
    "path4=\"y_val.csv\"\n",
    "path5=\"X_test.csv\"\n",
    "path6=\"y_test.csv\"\n",
    "\n",
    "X_train=pd.read_csv(path1)\n",
    "y_train=pd.read_csv(path2)\n",
    "X_val=pd.read_csv(path3)\n",
    "y_val=pd.read_csv(path4)\n",
    "X_test=pd.read_csv(path5)\n",
    "y_test=pd.read_csv(path6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a arquitetura de DNN, a estratégia passou por permitir ter uma abordagem rectangular onde o numero de neurónio nas diferentes hidden layers é o mesmo, ou uma abordagem cónica onde em cada hidden layer o numero de neurónios é metade do numero de neurónios da camada anterior.\n",
    "\n",
    "Realizamos esta abordagem apenas para um máximo de 3 camadas (hidden) tendo em conta que computacionalemente é pesado, mas tendo perfeita noção de que poderiamos aumentar o numero de camadas.\n",
    "\n",
    "Além disso verificamos o comportamento com taza de dropout de 0.2 e 0.5 bem como eraling stopping de 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m3/zkz1w1516xd9qw8bldx6sp780000gn/T/ipykernel_7287/1241221367.py:28: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, epochs=25, batch_size=32, verbose=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 4ms/step - loss: 269.9479 - val_loss: 228.9121\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 228.4473 - val_loss: 218.5089\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 211.9869 - val_loss: 210.6485\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 198.9236 - val_loss: 198.6761\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 187.8943 - val_loss: 197.4998\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 179.9882 - val_loss: 185.9579\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 172.2811 - val_loss: 192.9575\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 165.5193 - val_loss: 184.1445\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 160.8766 - val_loss: 182.7797\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 155.0684 - val_loss: 178.7348\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 149.1810 - val_loss: 179.2354\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 145.4947 - val_loss: 181.2155\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 268.8627 - val_loss: 232.0380\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 231.0298 - val_loss: 221.9307\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 212.1750 - val_loss: 210.2899\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 195.8118 - val_loss: 195.9441\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 180.9623 - val_loss: 196.9024\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 176.7395 - val_loss: 186.7135\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 171.2479 - val_loss: 181.7859\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 161.6044 - val_loss: 196.1546\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 157.0954 - val_loss: 178.0248\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 154.0680 - val_loss: 178.0054\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 148.1372 - val_loss: 176.3371\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 145.3549 - val_loss: 183.1968\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 140.5528 - val_loss: 170.9928\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 137.1000 - val_loss: 176.6785\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 135.5495 - val_loss: 174.6028\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 272.5747 - val_loss: 236.5162\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 236.8465 - val_loss: 217.3573\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 215.3536 - val_loss: 213.4363\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 200.6264 - val_loss: 202.9397\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 191.7831 - val_loss: 195.1849\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 182.6698 - val_loss: 190.3734\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 172.6179 - val_loss: 185.4720\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 169.5920 - val_loss: 182.0247\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 161.2350 - val_loss: 180.6123\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 158.0181 - val_loss: 174.8627\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 151.2760 - val_loss: 175.6329\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 148.3789 - val_loss: 173.7636\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 143.0344 - val_loss: 170.8469\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 142.2851 - val_loss: 176.2161\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 141.3972 - val_loss: 170.0461\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 136.6639 - val_loss: 170.1155\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 131.8859 - val_loss: 170.3570\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 279.9693 - val_loss: 239.4066\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 250.6736 - val_loss: 222.5201\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 227.6138 - val_loss: 219.7691\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 217.7665 - val_loss: 220.2062\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 207.1714 - val_loss: 210.1057\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 197.3527 - val_loss: 197.6248\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 189.0685 - val_loss: 195.7765\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 181.7630 - val_loss: 195.2935\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 179.0911 - val_loss: 197.7250\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 174.0235 - val_loss: 188.8372\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 167.8121 - val_loss: 190.0855\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 162.6478 - val_loss: 180.8153\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 158.4750 - val_loss: 179.8568\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 153.4580 - val_loss: 182.0951\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 150.5199 - val_loss: 177.2343\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 149.3620 - val_loss: 177.5640\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 143.9029 - val_loss: 186.6240\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 4ms/step - loss: 280.8536 - val_loss: 253.1302\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 249.7665 - val_loss: 228.8546\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 229.2320 - val_loss: 221.3063\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 217.0234 - val_loss: 218.3977\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 205.2794 - val_loss: 209.2215\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 196.2924 - val_loss: 205.6966\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 191.3646 - val_loss: 194.6637\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 179.7303 - val_loss: 202.1004\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 175.8158 - val_loss: 188.9631\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 172.2628 - val_loss: 183.0297\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 165.8676 - val_loss: 187.1830\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 163.1559 - val_loss: 184.2924\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 280.5107 - val_loss: 244.5410\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 246.3929 - val_loss: 225.1768\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 232.3036 - val_loss: 229.7755\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 219.4753 - val_loss: 210.2179\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 210.2366 - val_loss: 206.9013\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 199.3284 - val_loss: 198.2234\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 192.8116 - val_loss: 201.6428\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 186.2201 - val_loss: 203.2882\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 268.2036 - val_loss: 231.4985\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 225.0441 - val_loss: 219.0312\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 200.2005 - val_loss: 201.2226\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 186.0855 - val_loss: 201.3501\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 172.3541 - val_loss: 189.3169\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 163.2981 - val_loss: 191.0443\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 152.5540 - val_loss: 188.7362\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 145.9468 - val_loss: 175.3728\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 140.5807 - val_loss: 177.5859\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 134.6013 - val_loss: 176.2998\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 267.8661 - val_loss: 228.6528\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 224.0903 - val_loss: 212.2065\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 202.2533 - val_loss: 204.2120\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 182.4189 - val_loss: 194.4443\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 172.0345 - val_loss: 186.7270\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 162.2467 - val_loss: 185.3353\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 155.6150 - val_loss: 183.9649\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 151.0367 - val_loss: 179.2088\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 144.0605 - val_loss: 173.9323\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 139.4639 - val_loss: 169.2092\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 130.6814 - val_loss: 171.8311\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 131.2824 - val_loss: 170.8450\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 272.9848 - val_loss: 222.8931\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 223.0181 - val_loss: 212.2210\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 201.0066 - val_loss: 203.3406\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 186.9973 - val_loss: 186.2107\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 174.4130 - val_loss: 189.5548\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 161.9706 - val_loss: 184.3982\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 156.3781 - val_loss: 182.8094\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 149.2171 - val_loss: 177.6492\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 139.7188 - val_loss: 172.3011\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 140.1881 - val_loss: 168.5128\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 134.0973 - val_loss: 169.4415\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 129.3163 - val_loss: 165.8816\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 124.4102 - val_loss: 164.1902\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 122.0735 - val_loss: 166.6671\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 118.4925 - val_loss: 164.1087\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 116.3387 - val_loss: 169.5150\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 113.2230 - val_loss: 169.3553\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 276.5362 - val_loss: 257.6808\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 237.3913 - val_loss: 226.4428\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 221.0845 - val_loss: 220.0850\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 209.7832 - val_loss: 227.0347\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 197.1795 - val_loss: 199.4289\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 184.8192 - val_loss: 198.9085\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 175.1333 - val_loss: 188.5301\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 171.5000 - val_loss: 190.6217\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 159.3654 - val_loss: 177.2725\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 152.4062 - val_loss: 178.8397\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 145.7195 - val_loss: 180.3471\n",
      "143/143 [==============================] - 0s 3ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 277.2243 - val_loss: 242.0062\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 239.0535 - val_loss: 226.1579\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 215.4115 - val_loss: 225.0592\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 207.8466 - val_loss: 212.4004\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 192.1423 - val_loss: 201.0543\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 183.6101 - val_loss: 193.3783\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 174.8758 - val_loss: 192.5040\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 165.9951 - val_loss: 178.5118\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 160.4956 - val_loss: 176.8476\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 154.4784 - val_loss: 179.0027\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 148.9265 - val_loss: 176.6400\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 144.5931 - val_loss: 177.4063\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 141.4783 - val_loss: 178.5871\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 280.2995 - val_loss: 230.0744\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 237.3950 - val_loss: 221.9936\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 221.4848 - val_loss: 207.6893\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 207.0089 - val_loss: 203.5512\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 196.0177 - val_loss: 190.9535\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 184.0135 - val_loss: 186.5024\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 172.9442 - val_loss: 190.4170\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 166.3931 - val_loss: 187.2600\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 269.9093 - val_loss: 235.6025\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 231.2718 - val_loss: 219.4547\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 211.3262 - val_loss: 212.8178\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 194.8789 - val_loss: 202.2723\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 187.1002 - val_loss: 194.5140\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 179.6372 - val_loss: 193.0523\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 168.8872 - val_loss: 186.5761\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 162.9064 - val_loss: 183.6397\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 155.4899 - val_loss: 186.0732\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 152.8279 - val_loss: 177.7603\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 147.4960 - val_loss: 177.1999\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 142.4805 - val_loss: 178.0939\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 138.3085 - val_loss: 171.9787\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 133.0404 - val_loss: 179.6637\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 133.4644 - val_loss: 171.2096\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 129.9551 - val_loss: 168.4686\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 128.6585 - val_loss: 174.0552\n",
      "Epoch 18/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 122.6227 - val_loss: 169.4282\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 266.6225 - val_loss: 241.6708\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 229.4092 - val_loss: 212.3132\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 209.4175 - val_loss: 206.4305\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 194.6665 - val_loss: 212.4812\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 186.5520 - val_loss: 194.8494\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 168.3866 - val_loss: 188.1700\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 168.7203 - val_loss: 182.5967\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 156.5961 - val_loss: 181.2100\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 152.1406 - val_loss: 178.7450\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 150.5592 - val_loss: 175.3636\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 144.7699 - val_loss: 176.7395\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 142.0531 - val_loss: 176.7890\n",
      "143/143 [==============================] - 0s 1ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 272.7969 - val_loss: 240.9559\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 235.4069 - val_loss: 213.0223\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 215.7439 - val_loss: 207.4489\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 199.7656 - val_loss: 202.0574\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 189.7690 - val_loss: 189.9606\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 177.3887 - val_loss: 190.2780\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 167.8627 - val_loss: 188.7503\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 166.4480 - val_loss: 183.7960\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 155.6229 - val_loss: 182.4847\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 148.8729 - val_loss: 178.7872\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 150.5130 - val_loss: 177.0131\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 3s 10ms/step - loss: 144.7267 - val_loss: 180.5371\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 136.9503 - val_loss: 175.5226\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 135.8955 - val_loss: 173.6169\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 131.3224 - val_loss: 175.4409\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 130.2776 - val_loss: 174.5740\n",
      "143/143 [==============================] - 0s 1ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 265.2423 - val_loss: 246.6176\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 237.7674 - val_loss: 229.8670\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 222.7668 - val_loss: 224.9814\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 203.3743 - val_loss: 209.8521\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 193.8386 - val_loss: 202.3048\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 185.5383 - val_loss: 199.1300\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 181.4367 - val_loss: 191.8526\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 169.4203 - val_loss: 202.8386\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 161.4972 - val_loss: 188.8382\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 153.4958 - val_loss: 199.1844\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 154.4830 - val_loss: 187.2600\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 149.5671 - val_loss: 188.6220\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 143.1466 - val_loss: 186.5040\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 139.8486 - val_loss: 182.2554\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 134.6290 - val_loss: 177.2767\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 132.4780 - val_loss: 185.2810\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 132.3792 - val_loss: 181.0709\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 271.2380 - val_loss: 245.8962\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 235.4535 - val_loss: 222.2510\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 217.9730 - val_loss: 209.4870\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 206.5792 - val_loss: 211.7459\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 196.6045 - val_loss: 201.0641\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 186.4407 - val_loss: 199.2943\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 176.0838 - val_loss: 189.6638\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 172.7829 - val_loss: 190.3766\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 165.3562 - val_loss: 191.1444\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 271.8781 - val_loss: 233.3420\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 241.3991 - val_loss: 227.6424\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 223.9079 - val_loss: 212.9975\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 212.0397 - val_loss: 217.3197\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 199.9071 - val_loss: 215.0549\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 267.2687 - val_loss: 244.2355\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 221.3197 - val_loss: 212.3459\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 198.9081 - val_loss: 203.1688\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 185.0785 - val_loss: 197.9482\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 170.1791 - val_loss: 182.7508\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 162.5387 - val_loss: 183.4494\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 153.6340 - val_loss: 191.3954\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 264.8874 - val_loss: 221.7768\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 217.8485 - val_loss: 209.0301\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 191.8130 - val_loss: 203.7341\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 179.2055 - val_loss: 184.5534\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 167.5919 - val_loss: 188.9005\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 155.7805 - val_loss: 199.2336\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 268.9530 - val_loss: 231.4808\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 219.2375 - val_loss: 212.1084\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 200.2832 - val_loss: 192.3020\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 184.1625 - val_loss: 199.1865\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 172.0426 - val_loss: 178.7228\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 161.8474 - val_loss: 178.8133\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 151.4657 - val_loss: 178.4285\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 142.8519 - val_loss: 184.1211\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 139.9615 - val_loss: 174.9883\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 134.4598 - val_loss: 180.9359\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 128.6530 - val_loss: 168.4291\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 123.6350 - val_loss: 175.8722\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 117.5122 - val_loss: 172.8233\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 267.3070 - val_loss: 238.7094\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 232.5592 - val_loss: 216.0760\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 215.7355 - val_loss: 212.3811\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 199.2915 - val_loss: 194.8732\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 184.7939 - val_loss: 203.3517\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 171.7619 - val_loss: 191.6009\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 164.0065 - val_loss: 193.7079\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 155.5758 - val_loss: 184.5490\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 148.2851 - val_loss: 183.4869\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 141.3746 - val_loss: 176.8229\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 136.9403 - val_loss: 177.9751\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 131.6240 - val_loss: 170.1860\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 125.5311 - val_loss: 173.3701\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 123.5265 - val_loss: 172.1944\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 268.2754 - val_loss: 236.3049\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 232.5219 - val_loss: 223.3883\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 211.7257 - val_loss: 206.9694\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 192.6258 - val_loss: 209.7390\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 180.0067 - val_loss: 198.9155\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 172.4507 - val_loss: 199.6040\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 162.9869 - val_loss: 192.7297\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 148.9012 - val_loss: 181.5456\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 148.5329 - val_loss: 177.6281\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 142.3544 - val_loss: 174.5260\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 135.5933 - val_loss: 170.7337\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 132.8580 - val_loss: 175.3510\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 125.6265 - val_loss: 174.5514\n",
      "143/143 [==============================] - 0s 3ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 271.2885 - val_loss: 253.8960\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 233.1312 - val_loss: 210.5382\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 212.8160 - val_loss: 208.3146\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 196.1577 - val_loss: 206.0702\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 185.1915 - val_loss: 191.8754\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 172.4238 - val_loss: 188.3331\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 164.0674 - val_loss: 183.3765\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 156.8136 - val_loss: 179.5871\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 144.2498 - val_loss: 191.5543\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 140.6761 - val_loss: 180.7577\n",
      "143/143 [==============================] - 0s 3ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 312.9158 - val_loss: 260.5995\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 270.7647 - val_loss: 250.1087\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 253.8568 - val_loss: 250.6104\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 243.0320 - val_loss: 231.4241\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 236.3070 - val_loss: 230.2900\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 231.7406 - val_loss: 235.2480\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 228.1809 - val_loss: 225.3253\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 223.4122 - val_loss: 220.9175\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 219.6170 - val_loss: 219.9032\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 215.2113 - val_loss: 221.2296\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 213.1178 - val_loss: 219.5198\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 210.8822 - val_loss: 210.7836\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 203.2369 - val_loss: 206.4514\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 204.9025 - val_loss: 207.6205\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 203.0053 - val_loss: 213.8543\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 324.8360 - val_loss: 257.8909\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 270.6026 - val_loss: 251.7495\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 255.2274 - val_loss: 244.3654\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 242.9991 - val_loss: 235.8316\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 237.3028 - val_loss: 230.6704\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 231.7419 - val_loss: 226.4559\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 222.3597 - val_loss: 217.0161\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 218.3138 - val_loss: 221.4344\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 217.1402 - val_loss: 214.0976\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 210.8444 - val_loss: 212.7259\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 206.4050 - val_loss: 209.1852\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 207.4989 - val_loss: 202.6783\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 203.5459 - val_loss: 201.0012\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 198.5677 - val_loss: 208.9748\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 198.6302 - val_loss: 207.7225\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 4ms/step - loss: 305.9599 - val_loss: 261.8380\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 271.8470 - val_loss: 261.6235\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 255.5552 - val_loss: 248.0781\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 245.1265 - val_loss: 240.7187\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 237.9348 - val_loss: 227.9142\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 231.6021 - val_loss: 224.0484\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 223.5851 - val_loss: 227.2563\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 223.9336 - val_loss: 216.0051\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 220.5768 - val_loss: 213.4683\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 213.4937 - val_loss: 213.9546\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 211.4095 - val_loss: 215.6952\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 334.3049 - val_loss: 301.0888\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 294.8359 - val_loss: 272.2231\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 277.2818 - val_loss: 260.3353\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 263.7208 - val_loss: 261.8117\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 258.6698 - val_loss: 257.8485\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 250.1111 - val_loss: 253.5643\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 248.7220 - val_loss: 251.5595\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 245.7420 - val_loss: 236.7538\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 237.9073 - val_loss: 252.9824\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 233.1843 - val_loss: 232.7279\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 227.5682 - val_loss: 236.1370\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 229.2094 - val_loss: 226.4715\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 226.7684 - val_loss: 231.9109\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 222.6113 - val_loss: 215.3107\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 217.0639 - val_loss: 209.9106\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 217.0715 - val_loss: 217.2464\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 214.6019 - val_loss: 223.2477\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 345.2951 - val_loss: 295.8167\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 295.8730 - val_loss: 284.5034\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 275.8636 - val_loss: 257.4324\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 267.0070 - val_loss: 266.4950\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 259.3283 - val_loss: 255.3330\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 256.4035 - val_loss: 240.7175\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 248.8795 - val_loss: 243.5425\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 244.2492 - val_loss: 238.8164\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 236.3898 - val_loss: 244.2509\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 237.8109 - val_loss: 227.4661\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 232.2496 - val_loss: 235.0099\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 223.5927 - val_loss: 216.4087\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 224.6128 - val_loss: 214.1174\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 220.9867 - val_loss: 226.8598\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 218.1520 - val_loss: 218.9155\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 343.2572 - val_loss: 294.2742\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 302.2145 - val_loss: 275.3531\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 281.9068 - val_loss: 274.1981\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 268.7754 - val_loss: 258.8431\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 262.9182 - val_loss: 248.0704\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 255.4794 - val_loss: 244.8450\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 253.0466 - val_loss: 234.7433\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 251.5070 - val_loss: 239.7205\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 244.0924 - val_loss: 235.0199\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 303.6070 - val_loss: 255.7308\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 261.0836 - val_loss: 244.4174\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 246.3761 - val_loss: 240.3281\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 235.1255 - val_loss: 238.6235\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 223.6327 - val_loss: 227.3499\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 222.5989 - val_loss: 215.8752\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 214.8083 - val_loss: 220.9021\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 210.6706 - val_loss: 213.2306\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 203.5202 - val_loss: 214.2068\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 203.3561 - val_loss: 206.9047\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 199.6205 - val_loss: 199.9221\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 192.6700 - val_loss: 200.8578\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 189.4062 - val_loss: 193.6173\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 186.2336 - val_loss: 195.2777\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 186.1340 - val_loss: 193.4521\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 183.8286 - val_loss: 196.7618\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 177.9685 - val_loss: 209.4179\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 311.9810 - val_loss: 259.0686\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 258.9580 - val_loss: 247.4037\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 246.5120 - val_loss: 238.3308\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 231.8527 - val_loss: 229.2748\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 223.4747 - val_loss: 217.7605\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 217.8549 - val_loss: 211.1892\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 211.4610 - val_loss: 219.4760\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 207.0106 - val_loss: 215.0399\n",
      "143/143 [==============================] - 1s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 316.5418 - val_loss: 244.4077\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 264.1862 - val_loss: 234.1189\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 248.0610 - val_loss: 233.4484\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 235.3233 - val_loss: 234.7810\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 227.6655 - val_loss: 224.8907\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 225.3025 - val_loss: 216.2908\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 218.2184 - val_loss: 221.4837\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 213.4939 - val_loss: 212.3315\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 208.1670 - val_loss: 210.7841\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 205.7710 - val_loss: 205.9527\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 195.9302 - val_loss: 208.9461\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 5ms/step - loss: 196.2634 - val_loss: 190.1259\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 194.2081 - val_loss: 195.2500\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 194.7693 - val_loss: 195.4921\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 341.5442 - val_loss: 275.8864\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 287.3571 - val_loss: 279.3188\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 266.1252 - val_loss: 271.1107\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 255.4319 - val_loss: 250.2160\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 249.5217 - val_loss: 238.9352\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 242.6532 - val_loss: 243.5496\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 233.7501 - val_loss: 247.8295\n",
      "143/143 [==============================] - 0s 3ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 339.8809 - val_loss: 284.5858\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 290.8709 - val_loss: 277.7132\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 265.4334 - val_loss: 274.4750\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 255.6405 - val_loss: 239.9008\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 249.6976 - val_loss: 257.4749\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 243.0522 - val_loss: 232.6201\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 233.4097 - val_loss: 240.8362\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 229.7758 - val_loss: 240.5729\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 339.0205 - val_loss: 285.9291\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 283.9780 - val_loss: 268.1645\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 268.4984 - val_loss: 261.3588\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 259.7824 - val_loss: 241.6563\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 250.2829 - val_loss: 247.1055\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 243.9037 - val_loss: 244.8492\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 309.1618 - val_loss: 254.6170\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 264.0410 - val_loss: 232.6348\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 251.3811 - val_loss: 245.0287\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 236.7536 - val_loss: 231.0052\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 231.5413 - val_loss: 237.4548\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 225.2236 - val_loss: 218.8123\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 216.3024 - val_loss: 217.9536\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 217.2300 - val_loss: 205.8326\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 208.8840 - val_loss: 216.7563\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 206.9399 - val_loss: 208.7654\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 305.4726 - val_loss: 251.7491\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 264.2954 - val_loss: 237.5827\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 249.4442 - val_loss: 243.9577\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 237.3359 - val_loss: 233.4987\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 229.8184 - val_loss: 224.4047\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 220.9052 - val_loss: 235.1068\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 218.2281 - val_loss: 219.1189\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 212.0752 - val_loss: 227.9101\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 207.5292 - val_loss: 208.3495\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 207.1381 - val_loss: 199.6913\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 201.2103 - val_loss: 212.2180\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 198.4024 - val_loss: 216.9162\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 313.8370 - val_loss: 260.5695\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 263.3022 - val_loss: 238.8175\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 251.1118 - val_loss: 234.7685\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 239.7364 - val_loss: 238.3060\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 230.1974 - val_loss: 223.4293\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 226.1266 - val_loss: 218.7200\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 218.0183 - val_loss: 220.8449\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 210.8836 - val_loss: 209.5549\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 211.8568 - val_loss: 224.4118\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 210.1230 - val_loss: 213.4985\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 319.0415 - val_loss: 283.0203\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 277.4867 - val_loss: 268.3587\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 262.9965 - val_loss: 276.9261\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 251.2508 - val_loss: 261.3133\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 245.6617 - val_loss: 244.2421\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 234.5727 - val_loss: 239.2433\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 234.7963 - val_loss: 238.0038\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 228.5018 - val_loss: 244.5681\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 224.6961 - val_loss: 244.7734\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 327.4573 - val_loss: 268.3250\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 276.7255 - val_loss: 280.6111\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 261.2197 - val_loss: 267.0416\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 247.0570 - val_loss: 265.1787\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 242.1761 - val_loss: 245.4855\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 238.0828 - val_loss: 249.8554\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 232.1123 - val_loss: 252.9503\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 323.6178 - val_loss: 269.5425\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 280.4099 - val_loss: 268.7278\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 263.1890 - val_loss: 254.1606\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 248.8146 - val_loss: 248.5952\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 243.5170 - val_loss: 247.4518\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 237.5484 - val_loss: 251.7158\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 236.7875 - val_loss: 233.1996\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 232.3658 - val_loss: 235.5301\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 222.6930 - val_loss: 230.1809\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 222.7733 - val_loss: 248.9793\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 217.5323 - val_loss: 229.0714\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 213.6237 - val_loss: 230.4812\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 210.4622 - val_loss: 221.1190\n",
      "Epoch 14/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 208.0861 - val_loss: 219.0413\n",
      "Epoch 15/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 206.3104 - val_loss: 220.4486\n",
      "Epoch 16/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 201.2511 - val_loss: 207.0818\n",
      "Epoch 17/25\n",
      "286/286 [==============================] - 1s 4ms/step - loss: 201.2917 - val_loss: 209.8647\n",
      "Epoch 18/25\n",
      "286/286 [==============================] - 1s 3ms/step - loss: 196.1998 - val_loss: 218.3775\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 310.2146 - val_loss: 272.1161\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 256.5062 - val_loss: 242.8460\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 242.2357 - val_loss: 233.5945\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 227.4440 - val_loss: 223.2095\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 221.3017 - val_loss: 211.9577\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 219.7041 - val_loss: 223.6437\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 209.0120 - val_loss: 227.1862\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 310.9418 - val_loss: 257.4762\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 254.4879 - val_loss: 239.1630\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 240.2238 - val_loss: 237.9410\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 229.8675 - val_loss: 230.0628\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 221.8147 - val_loss: 221.9556\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 211.7225 - val_loss: 224.5294\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 205.1869 - val_loss: 217.4270\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 5ms/step - loss: 202.8837 - val_loss: 220.6755\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 195.6821 - val_loss: 231.3218\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 2s 8ms/step - loss: 314.3991 - val_loss: 266.4599\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 260.5956 - val_loss: 244.1313\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 242.7181 - val_loss: 236.9782\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 231.4194 - val_loss: 221.7923\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 223.3424 - val_loss: 218.6883\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 217.7917 - val_loss: 215.4001\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 210.7801 - val_loss: 219.3643\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 204.3856 - val_loss: 210.3963\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 201.4527 - val_loss: 216.1447\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 197.4983 - val_loss: 214.2539\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 3s 7ms/step - loss: 318.6903 - val_loss: 257.9626\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 268.3208 - val_loss: 261.9648\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 253.5543 - val_loss: 257.9081\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 242.1161 - val_loss: 243.6801\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 234.9592 - val_loss: 233.0738\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 227.8531 - val_loss: 234.7720\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 223.4011 - val_loss: 221.2798\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 219.7569 - val_loss: 226.4552\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 212.0737 - val_loss: 217.5893\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 208.7309 - val_loss: 215.1973\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 202.1184 - val_loss: 202.8102\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 202.5466 - val_loss: 231.2290\n",
      "Epoch 13/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 198.4526 - val_loss: 217.8050\n",
      "143/143 [==============================] - 0s 3ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 3s 8ms/step - loss: 318.9385 - val_loss: 252.8319\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 271.8204 - val_loss: 266.3812\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 255.2829 - val_loss: 246.9783\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 242.7812 - val_loss: 260.6847\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 235.5103 - val_loss: 242.2423\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 229.4916 - val_loss: 237.0081\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 222.0240 - val_loss: 221.4741\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 218.2005 - val_loss: 218.2799\n",
      "Epoch 9/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 214.7565 - val_loss: 218.6503\n",
      "Epoch 10/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 207.5337 - val_loss: 206.6422\n",
      "Epoch 11/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 200.5639 - val_loss: 213.5398\n",
      "Epoch 12/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 201.8437 - val_loss: 228.2349\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "286/286 [==============================] - 3s 8ms/step - loss: 324.6594 - val_loss: 261.2040\n",
      "Epoch 2/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 274.0630 - val_loss: 270.2645\n",
      "Epoch 3/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 257.7856 - val_loss: 243.5308\n",
      "Epoch 4/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 245.1599 - val_loss: 235.7732\n",
      "Epoch 5/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 238.8501 - val_loss: 233.2159\n",
      "Epoch 6/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 235.8073 - val_loss: 231.3364\n",
      "Epoch 7/25\n",
      "286/286 [==============================] - 2s 6ms/step - loss: 227.5573 - val_loss: 242.2411\n",
      "Epoch 8/25\n",
      "286/286 [==============================] - 2s 7ms/step - loss: 219.6473 - val_loss: 239.4813\n",
      "143/143 [==============================] - 0s 2ms/step\n",
      "Epoch 1/25\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 261.4666 - val_loss: 229.2814\n",
      "Epoch 2/25\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 216.8485 - val_loss: 196.5151\n",
      "Epoch 3/25\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 192.4162 - val_loss: 189.2990\n",
      "Epoch 4/25\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 180.0936 - val_loss: 188.1737\n",
      "Epoch 5/25\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 168.3847 - val_loss: 171.8810\n",
      "Epoch 6/25\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 158.7406 - val_loss: 167.7345\n",
      "Epoch 7/25\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 151.7378 - val_loss: 170.7409\n",
      "Epoch 8/25\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 146.1646 - val_loss: 164.6907\n",
      "Epoch 9/25\n",
      "429/429 [==============================] - 2s 5ms/step - loss: 140.3520 - val_loss: 160.7834\n",
      "Epoch 10/25\n",
      "429/429 [==============================] - 2s 6ms/step - loss: 138.2635 - val_loss: 156.7642\n",
      "Epoch 11/25\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 134.2684 - val_loss: 159.4006\n",
      "Epoch 12/25\n",
      "429/429 [==============================] - 3s 6ms/step - loss: 128.8112 - val_loss: 172.1989\n",
      "Melhores Hiperparâmetros: {'callbacks': [<keras.callbacks.EarlyStopping object at 0x7fc9f57b7250>], 'dropout_rate': 0.2, 'layer_type': 'conic', 'neurons': 256, 'num_layers': 2, 'optimizer': 'adam'}\n",
      "92/92 [==============================] - 0s 2ms/step\n",
      "Desempenho no Conjunto de Teste: 0.4944\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def create_model(neurons=1, optimizer='adam', layer_type='rectangular', dropout_rate=0.0, num_layers=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    if layer_type == 'rectangular':\n",
    "        for _ in range(num_layers):\n",
    "            model.add(Dense(neurons, input_dim=X_train.shape[1], activation='relu'))\n",
    "            # Adicionar dropout se o dropout_rate for maior que 0\n",
    "            if dropout_rate > 0:\n",
    "                model.add(Dropout(dropout_rate))\n",
    "    elif layer_type == 'conic':\n",
    "        for _ in range(num_layers):\n",
    "            model.add(Dense(neurons, input_dim=X_train.shape[1], activation='relu'))\n",
    "            if dropout_rate > 0:\n",
    "                model.add(Dropout(dropout_rate))\n",
    "            neurons=neurons //2\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Criar o modelo KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_model, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "# Definir os hiperparâmetros a serem testados\n",
    "parameters = {\n",
    "    'neurons': [128,256],\n",
    "    'optimizer': ['adam'],\n",
    "    'layer_type': ['conic','rectangular'],\n",
    "    'dropout_rate': [0.2,0.5],\n",
    "    'num_layers': [2,3],  # Adicionado para escolher entre 2 ou 3 camadas\n",
    "    'callbacks': [[EarlyStopping(monitor='val_loss', patience=2)]]\n",
    "}\n",
    "\n",
    "# Criar objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring='r2', cv=3)\n",
    "\n",
    "# Treinar o modelo usando a busca em grade\n",
    "grid_result = grid_search.fit(X_train, y_train, validation_data=(X_val, y_val))\n",
    "\n",
    "# Exibir os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores Hiperparâmetros:\", grid_result.best_params_)\n",
    "\n",
    "# Avaliar o modelo no conjunto de teste\n",
    "test_score = grid_result.score(X_test, y_test)\n",
    "print(f\"Desempenho no Conjunto de Teste: {test_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o CNN realizamos um modelo baseline, uma vez que devido ao poder computacional que possuimos o uso de hiperparametros seria demasiado demorado.\n",
    "\n",
    "Se o indicado anteriormente não fosse um problema, para a arquitetura de CNN, a estratégia passaria por permitir escolher o kernel_size, bem como o numero de camadas Conv/pooling (maximo de 3), além disso após a realização do flatten a DNN também poderia ser realizada com um máximo de 3 camadas.\n",
    "\n",
    "Além disso verificamos o comportamento com taza de dropout de 0.2 e 0.5 bem como eraling stopping de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tiago_silva/opt/anaconda3/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "429/429 [==============================] - 14s 30ms/step - loss: 259.2995 - val_loss: 242.5519\n",
      "Epoch 2/10\n",
      "429/429 [==============================] - 11s 26ms/step - loss: 246.2193 - val_loss: 244.3100\n",
      "Epoch 3/10\n",
      "429/429 [==============================] - 11s 25ms/step - loss: 245.2748 - val_loss: 242.4518\n",
      "Epoch 4/10\n",
      "429/429 [==============================] - 14s 33ms/step - loss: 244.0597 - val_loss: 240.9978\n",
      "Epoch 5/10\n",
      "429/429 [==============================] - 13s 30ms/step - loss: 244.1934 - val_loss: 240.8664\n",
      "Epoch 6/10\n",
      "429/429 [==============================] - 11s 25ms/step - loss: 243.1860 - val_loss: 240.1744\n",
      "Epoch 7/10\n",
      "429/429 [==============================] - 10s 24ms/step - loss: 243.1309 - val_loss: 241.7224\n",
      "Epoch 8/10\n",
      "429/429 [==============================] - 11s 24ms/step - loss: 243.1982 - val_loss: 239.1278\n",
      "Epoch 9/10\n",
      "429/429 [==============================] - 10s 24ms/step - loss: 242.8004 - val_loss: 240.7306\n",
      "Epoch 10/10\n",
      "429/429 [==============================] - 11s 25ms/step - loss: 242.7011 - val_loss: 239.2269\n",
      "92/92 [==============================] - 1s 8ms/step\n",
      "R-squared score on test data:  0.25448990254728476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared score on test data: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O valor obtido de r2 é inferior ao obtido pela arquitetura do DNN, apresentado a cima. Mas tambem queremos realçar que o número de epochs é bastante inferior e achamos que caso fosse possivel computacionalmente realizar o codigo abaixo, que o valor de r2 seria superior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Função para criar o modelo CNN\n",
    "def create_cnn_model(kernel_size=3, filters=32, dense_units=128, dense_layers=2, dropout_rate=0.5, num_layers=3):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters, kernel_size=kernel_size, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    filters=filters //2\n",
    "    for _ in range(num_layers - 1):  # Adiciona camadas extras\n",
    "        model.add(Conv1D(filters, kernel_size=kernel_size, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        filters=filters //2\n",
    "    model.add(Flatten())\n",
    "\n",
    "    for _ in range(dense_layers):  # Adiciona camadas densas\n",
    "        model.add(Dense(dense_units, activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(), metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "# Crie um modelo KerasRegressor\n",
    "model = KerasRegressor(build_fn=create_cnn_model, epochs=25, batch_size=32, verbose=1)\n",
    "\n",
    "# Defina os hiperparâmetros a serem testados\n",
    "parameters = {\n",
    "    'kernel_size': [3,5],\n",
    "    'filters': [32,64],\n",
    "    'dense_units': [128,256],\n",
    "    'dense_layers': [2,3],  # Adicionado para escolher entre 2 ou 3 camadas densas\n",
    "    'dropout_rate': [0.2,0.5],\n",
    "    'num_layers': [2,3],\n",
    "    'callbacks': [[EarlyStopping(monitor='val_loss', patience=2)]]\n",
    "}\n",
    "\n",
    "# Crie um objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=parameters, scoring='r2', cv=3)\n",
    "\n",
    "# Treine o modelo usando a busca em grade\n",
    "grid_result = grid_search.fit(X_train, y_train, validation_data=(X_val, y_val))\n",
    "\n",
    "# Exiba os melhores hiperparâmetros encontrados\n",
    "print(\"Melhores Hiperparâmetros:\", grid_result.best_params_)\n",
    "\n",
    "# Avalie o modelo no conjunto de teste\n",
    "test_score = grid_result.score(X_test, y_test)\n",
    "print(f\"Desempenho no Conjunto de Teste: {test_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concluímos, então, que o melhor modelo treinado entre todos os que testamos foi o Random Forest, apesar de a arquitetura de DNN apresentar um score bastante semelhante.\n",
    "\n",
    "Embora saibamos que poderíamos obter melhores resultados nos modelos que utilizam as arquiteturas de DNN e o CNN se tivéssemos mais recursos/poder computacional para os suportar, porque, apesar de ser exequível, demoraria dias para testarmos tudo o que queríamos.\n",
    "\n",
    "Concluímos também que, apesar de estarmos a usar um dataset que foi utilizado na literatura, conseguimos treinar ou afinar melhor os modelos para que estes apresentassem resultados melhores do que os apresentados no artigo, \"DeepSynergy: predicting anti-cancer drug synergy with Deep Learning\", em que nos baseamos.\n",
    "\n",
    "Sabemos também que haveria a possibilidade de obtermos melhores resultados, através do já mencionado maior poder computacional, e se tivéssemos mais combinações de drogas para treinar os modelos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
