{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f95ad60",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0592ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pprint import pprint\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba56b0ccbec712fe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Divisão do dataset em treino e em teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6117480d9ae682c9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataset.drop('Y', axis=1)\n",
    "y = dataset['Y']\n",
    "\n",
    "train_X, train_y, test_X, test_y= train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0245b37f0844d",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed266105c8cfed50",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Modelos de ML\n",
    "A nossa task, prever o nivel de sinergia entre duas drogas, o que se trata de um problema de regressão, logo só poderemos utilizar modelos que se baseiem em regressão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7169fca407762ef1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Logistical Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f2c05",
   "metadata": {},
   "source": [
    "Um modelo de regressão logística modela a relação entre as variáveis independentes e a probabilidade de pertencer a uma determinada classe usando uma função logística. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805998d",
   "metadata": {},
   "source": [
    "Aqui os hiperparâmetros estão definidos, mas podiamos fazer algo para determinar a melhro combinação.\n",
    "\n",
    "Depois de cada modelo já feito com parâmetreos definidos teoricamente, tem outro exemplo com a exploração dos melhores hiperparâmetros. Para discutir a melhor abordagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c6b13e5198d9f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=45, max_iter=1000)\n",
    "logreg.fit(train_X, train_y)\n",
    "\n",
    "print(f'Accuracy: {logreg.score(test_X, test_y):.2%}')\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "pred_y = logreg.predict(test_X)\n",
    "pprint(classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a1d0de3024b60",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix:')\n",
    "confusion_matrix(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d69996aecbb78",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define metrics\n",
    "y_pred_proba = logreg.predict_proba(test_y)[::,1]\n",
    "fpr, tpr, _ = roc_curve(test_y,  y_pred_proba)\n",
    "auc = roc_auc_score(test_y, y_pred_proba)\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569e8d9",
   "metadata": {},
   "source": [
    "#### Com exploração de hiperparâmetros para determinar a melhor combinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d78cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os hiperparâmetros \n",
    "parameters = {\n",
    "    'penalty': ['l1', 'l2', 'none'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'],\n",
    "    'max_iter': [10,100, 200]\n",
    "}\n",
    "\n",
    "# Criar o modelo de regressão logística\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(logistic_regression, parameters, scoring='f1', cv=5)\n",
    "\n",
    "# Executar a busca em grid para encontrar os melhores hiperparâmetros\n",
    "grid_search.fit(train_X[:, 3:], train_y)  #FUI USANDO SEMPRE [:, 3:] (todas as linhas e descartando as 3 primeiras colunas) MAS NÃO ESTOU A VER NO DATASET QUAIS COLUNAS INCLUIR, POR ISSO QUANDO SE TIVER O DATASET TEM QUE SE ADAPTAR ISTO EM TODOS ABAIXO\n",
    "\n",
    "# Obter os melhores hiperparâmetros encontrados\n",
    "best_params_LT = grid_search.best_params_\n",
    "best_params_LT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e313403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREINO\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Supondo que você tenha um conjunto de dados train_X, train_y\n",
    "# e que 'best_params_LT' contenha os melhores hiperparâmetros obtidos\n",
    "\n",
    "# Criar um objeto MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Criar o modelo Logistic Regression com os melhores hiperparâmetros\n",
    "LR_classifier = LogisticRegression(random_state=42, **best_params_LT)\n",
    "\n",
    "# Dividir os dados usando StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(train_X, train_y):\n",
    "    X_LR_train, X_LR_test, y_LR_train, y_LR_test =  train_X[train_index, 3:], train_X[test_index, 3:], train_y[train_index], train_y[test_index]\n",
    "    \n",
    "    # Normalizar os dados\n",
    "    X_LR_train_new = min_max_scaler.fit_transform(X_LR_train)\n",
    "    X_LR_test_new = min_max_scaler.transform(X_LR_test)\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    LR_classifier.fit(X_LR_train_new, y_LR_train)\n",
    "    \n",
    "    # Fazer previsões\n",
    "    y_pred = LR_classifier.predict(X_LR_test_new)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ad19da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTE\n",
    "import pandas as pd\n",
    "X_te_new = test_X[:,3:]\n",
    "x_test_df = pd.DataFrame(X_te_new)\n",
    "\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "x_test_df = pd.DataFrame(X_te_new)\n",
    "\n",
    "y_LR_pred = LR_classifier.predict(X_te_new)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_LR_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_LR_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_LR_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1c9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a matriz de confusão\n",
    "confusion = confusion_matrix(test_y, y_LR_pred)\n",
    "\n",
    "# Obtém os valores dos verdadeiros positivos (tp), verdadeiros negativos (tn),\n",
    "# falsos positivos (fp) e falsos negativos (fn)\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "# Imprime os valores da matriz de confusão\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Plota a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877cea12ab538e7d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decision Tree\n",
    "\n",
    "Árvore de decisão é um modelo que se assemelha a um fluxograma que toma decisões ou previsões, dividindo recursivamente os dados com base em diferentes carateristicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d130271582afea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtree = DecisionTreeRegressor(random_state=45)\n",
    "dtree.fit(train_X, train_y)\n",
    "\n",
    "print(f'Accuracy: {dtree.score(test_X, test_y):.2%}')\n",
    "\n",
    "pred_y = dtree.predict(test_X)\n",
    "print(classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7842f4",
   "metadata": {},
   "source": [
    "#### Com exploração de hiperparâmetros para determinar a melhor combinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b74c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hiperparametros\n",
    "parameters = {\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(dt_classifier, parameters, scoring='f1', cv=5)\n",
    "grid_search.fit(X_tr[:, 3:], train_y)\n",
    "\n",
    "best_params_DT = grid_search.best_params_\n",
    "best_params_DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ade0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREINO\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Supondo que você tenha um conjunto de dados train_X, train_y\n",
    "# e que 'best_params_DT' contenha os melhores hiperparâmetros obtidos\n",
    "\n",
    "# Criar um objeto MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Criar o modelo Decision Tree Classifier com os melhores hiperparâmetros\n",
    "DT_classifier = DecisionTreeClassifier(random_state=42, **best_params_DT)\n",
    "\n",
    "# Dividir os dados usando StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(train_X, train_y):\n",
    "    X_DT_train, X_DT_test, y_DT_train, y_DT_test = train_X[train_index, 3:], train_X[test_index, 3:], train_y[train_index], train_y[test_index]\n",
    "    \n",
    "    # Normalizar os dados\n",
    "    X_DT_train_new = min_max_scaler.fit_transform(X_DT_train)\n",
    "    X_DT_test_new = min_max_scaler.transform(X_DT_test)\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    DT_classifier.fit(X_DT_train_new, y_DT_train)\n",
    "    \n",
    "    # Fazer previsões\n",
    "    y_pred = DT_classifier.predict(X_DT_test_new)\n",
    "    \n",
    "    # Restante do seu código para avaliação do modelo, por exemplo, métricas de desempenho\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação\n",
    "\n",
    "import pandas as pd\n",
    "#AVALIAÇÃO\n",
    "X_te_new = X_te[:,3:]\n",
    "x_test_df = pd.DataFrame(X_te_new)\n",
    "\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "x_test_df = pd.DataFrame(X_te_new)\n",
    "\n",
    "y_DT_pred = DT_classifier.predict(X_te_new)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_DT_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_DT_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_DT_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "# Calcula a matriz de confusão\n",
    "confusion = confusion_matrix(y_te, y_DT_pred)\n",
    "\n",
    "# Obtém os valores dos verdadeiros positivos (tp), verdadeiros negativos (tn),\n",
    "# falsos positivos (fp) e falsos negativos (fn)\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "# Imprime os valores da matriz de confusão\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Plota Da matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d79bf703deb4d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Random Forest\n",
    "\n",
    "Random Forest é um método de aprendizagem que combina várias árvores de decisão para fazer previsões, e que suporta regressão e classificação. Este modelo pode não ser o mais adequado quando o dataset não é equilibrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf2d86d8631826b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestRegressor(random_state=45)\n",
    "rfc.fit(train_X, train_y)\n",
    "\n",
    "print(f'Accuracy: {rfc.score(test_X, test_y):.2%}')\n",
    "\n",
    "pred_y = rfc.predict(test_X)\n",
    "pprint(classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3a5e4e",
   "metadata": {},
   "source": [
    "#### Com exploração de hiperparâmetros para determinar a melhor combinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e77e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os parâmetros a ajustar\n",
    "parameters = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Criar o classificador RandomForest\n",
    "RF_classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(RF_classifier, parameters, scoring='f1', cv=5)\n",
    "\n",
    "# Executar o grid search usando os dados de treino (X_tr e y_tr)\n",
    "grid_search.fit(X_tr[:, 3:], y_tr)\n",
    "\n",
    "# Obter os melhores parâmetros\n",
    "best_params_RF = grid_search.best_params_\n",
    "best_params_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825ca5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TREINO\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Supondo que você tenha um conjunto de dados train_X, train_y\n",
    "# e que 'best_params_RF' contenha os melhores hiperparâmetros obtidos\n",
    "\n",
    "# Criar um objeto MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Criar o modelo RandomForestClassifier com os melhores hiperparâmetros\n",
    "RF_classifier = RandomForestClassifier(random_state=42, **best_params_RF)\n",
    "\n",
    "# Dividir os dados usando StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(train_X, train_y):\n",
    "    X_RF_train, X_RF_test, y_RF_train, y_RF_test = train_X[train_index, 3:], train_X[test_index, 3:], train_y[train_index], train_y[test_index]\n",
    "    \n",
    "    # Normalizar os dados\n",
    "    X_RF_train_new = min_max_scaler.fit_transform(X_RF_train)\n",
    "    X_RF_test_new = min_max_scaler.transform(X_RF_test)\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    RF_classifier.fit(X_RF_train_new, y_RF_train)\n",
    "    \n",
    "    # Fazer previsões\n",
    "    y_pred = RF_classifier.predict(X_RF_test_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4229dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação\n",
    "X_new = train_X[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "RF_classifier.fit(X_new, train_y)\n",
    "y_tr_predict = RF_classifier.predict(X_new)\n",
    "print('f1 on Train set: ', f1_score(y_tr, y_tr_predict))\n",
    "print('MCC on Train set: ', matthews_corrcoef(train_y, y_tr_predict))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_tr, y_tr_predict).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Train set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Train set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Train set: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teste\n",
    "X_te_new = X_te[:,3:]\n",
    "x_test_df = pd.DataFrame(X_te_new)\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "x_test_df = pd.DataFrame(X_te_new)\n",
    "y_RF_pred=RF_classifier.predict(X_te_new)\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_RF_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_RF_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_RF_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a6c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "# Calcula a matriz de confusão\n",
    "confusion = confusion_matrix(test_t, y_RF_pred)\n",
    "\n",
    "# Obtém os valores dos verdadeiros positivos (tp), verdadeiros negativos (tn),\n",
    "# falsos positivos (fp) e falsos negativos (fn)\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "# Imprime os valores da matriz de confusão\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Plot da matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a920e00bda91af84",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## SVR (Support Vector Regression)\n",
    "\n",
    "O SVR é uma técnica de ML para regressão, baseada em Support Vector Machines (SVM), que procura prever valores contínuos ao otimizar uma função de margem de erro entre as previsões do modelo e os valores reais dos dados de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39232e306108965",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svr = SVR()\n",
    "svr.fit(train_X, train_y)\n",
    "\n",
    "print(f'Accuracy: {svr.score(test_X, test_y):.2%}')\n",
    "\n",
    "pred_y = svr.predict(test_X)\n",
    "pprint(classification_report(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a96dd",
   "metadata": {},
   "source": [
    "#### Com exploração de hiperparâmetros para determinar a melhor combinação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os parâmetros a ajustar para SVR\n",
    "parameters_svr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'epsilon': [0.1, 0.2, 0.5]\n",
    "}\n",
    "\n",
    "# Criar o regressor SVR\n",
    "SVR_regressor = SVR()\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid_search_svr = GridSearchCV(SVR_regressor, parameters_svr, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Executar o grid search usando os dados de treino (X_tr e y_tr)\n",
    "grid_search.fit(train_X[:, 3:], train_y)\n",
    "\n",
    "# Obter os melhores parâmetros\n",
    "best_params_SVR = grid_search.best_params_\n",
    "best_params_SVR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f1a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TREINO\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Supondo que você tenha um conjunto de dados train_X, train_y\n",
    "# e que 'best_params_SVM' contenha os melhores hiperparâmetros obtidos\n",
    "\n",
    "# Criar um objeto MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# Definir os parâmetros a ajustar para SVM\n",
    "parameters_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.1, 1],\n",
    "    'degree': [2, 3, 4],\n",
    "}\n",
    "\n",
    "# Criar o classificador SVM\n",
    "SVM_classifier = SVC()\n",
    "\n",
    "# Criar o objeto GridSearchCV\n",
    "grid_search_svm = GridSearchCV(SVM_classifier, parameters_svm, scoring='accuracy', cv=5)\n",
    "\n",
    "# Dividir os dados usando StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_index, test_index in sss.split(train_X, train_y):\n",
    "    X_SVM_train, X_SVM_test, y_SVM_train, y_SVM_test = train_X[train_index, 3:], train_X[test_index, 3:], train_y[train_index], train_y[test_index]\n",
    "    \n",
    "    # Normalizar os dados\n",
    "    X_SVM_train_new = min_max_scaler.fit_transform(X_SVM_train)\n",
    "    X_SVM_test_new = min_max_scaler.transform(X_SVM_test)\n",
    "    \n",
    "    # Executar o grid search usando os dados de treino\n",
    "    grid_search_svm.fit(X_SVM_train_new, y_SVM_train)\n",
    "\n",
    "    # Obter os melhores parâmetros\n",
    "    best_params_SVM = grid_search_svm.best_params_\n",
    "    \n",
    "    # Criar o modelo SVM com os melhores hiperparâmetros\n",
    "    SVM_classifier = SVC(**best_params_SVM)\n",
    "    \n",
    "    # Treinar o modelo\n",
    "    SVM_classifier.fit(X_SVM_train_new, y_SVM_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac4566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Avaliação\n",
    "X_new = train_X[:,3:]\n",
    "X_new = min_max_scaler.fit_transform(X_new)\n",
    "KN_classifier.fit(X_new, y_tr)\n",
    "y_tr_predict = KN_classifier.predict(X_new)\n",
    "print('f1 on Train set: ', f1_score(y_tr, y_tr_predict))\n",
    "print('MCC on Train set: ', matthews_corrcoef(y_tr, y_tr_predict))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_tr, y_tr_predict).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn+fp)\n",
    "print('Specificity on Train set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp+fn)\n",
    "print('Sensitivity on Train set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp+tn) /(tp+tn+fp+fn)\n",
    "print('Accuracy on Train set: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27132d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTE\n",
    "\n",
    "X_te_new = test_X[:,3:]\n",
    "\n",
    "X_te_new = min_max_scaler.transform(X_te_new)\n",
    "\n",
    "y_SVM_pred = KN_classifier.predict(X_te_new)\n",
    "\n",
    "print(\"*************************************\")\n",
    "print('f1 on Test set: ', f1_score(y_te, y_SVM_pred))\n",
    "print('MCC on Test set: ', matthews_corrcoef(y_te, y_SVM_pred))\n",
    "tn, fp, fn, tp = confusion_matrix(y_te, y_SVM_pred).ravel()\n",
    "print(\"tn, fp, tp, fn\", tn, fp, tp, fn)\n",
    "specificity = tn / (tn + fp)\n",
    "print('Specificity on Test set(tn / (tn+fp)): ', specificity)\n",
    "sensitivity = tp / (tp + fn)\n",
    "print('Sensitivity on Test set(tp / (tp+fn)): ', sensitivity)\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print('Accuracy on Test set: ', accuracy)\n",
    "precision=tp/(tp+fp)\n",
    "print(\"Precision on Test set: \", precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d50c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix\n",
    "\n",
    "# Calcula a matriz de confusão\n",
    "confusion = confusion_matrix(y_te, y_SVM_pred)\n",
    "\n",
    "# Obtém os valores dos verdadeiros positivos (tp), verdadeiros negativos (tn),\n",
    "# falsos positivos (fp) e falsos negativos (fn)\n",
    "tn, fp, fn, tp = confusion.ravel()\n",
    "\n",
    "# Imprime os valores da matriz de confusão\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion)\n",
    "\n",
    "# Plota a matriz de confusão\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, cmap='Greens', fmt='g')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce14139f5ba20f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "IMPORTANTE: Podemos fazer como está em cima um de cada vez, ou entao, se quisermos treinar e testar todos os modelos de uma vez o prof usou isto na aula 10, ja pus os modelos que vi que deviamos usar, vejam se faz sentido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b95d767bd3e3b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = [LogisticRegression(random_state=42, max_iter=1000),\n",
    "          DecisionTreeRegressor(random_state=42),\n",
    "          RandomForestRegressor(random_state=42),\n",
    "          SVR()]\n",
    "\n",
    "for model in models:\n",
    "    model.fit(train_X, train_y)\n",
    "    print(model.__class__.__name__)\n",
    "    print('Accuracy on test set:', model.score(test_X, test_y))\n",
    "    print('Classification report:\\n', classification_report(test_y, model.predict(test_X)))\n",
    "    print('Confusion matrix:\\n', confusion_matrix(test_y, model.predict(test_X)))\n",
    "    print('ROC AUC score:', roc_auc_score(test_y, model.predict_proba(test_X)[::,1]))\n",
    "    print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a25621",
   "metadata": {},
   "source": [
    "## Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0885c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model validation\n",
    "\n",
    "# cross validation\n",
    "scores = cross_val_score(logreg, train_X, train_y, cv=5)\n",
    "print('Cross validation scores:', scores)\n",
    "print('Mean cross validation score:', scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfc6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap\n",
    "scores = []\n",
    "for i in range(1000):\n",
    "    X_boot, y_boot = resample(X_train, y_train)\n",
    "    logreg.fit(X_boot, y_boot)\n",
    "    scores.append(logreg.score(X_test, y_test))\n",
    "    \n",
    "print('Mean bootstrap score:', np.mean(scores))\n",
    "print('Standard deviation of bootstrap scores:', np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954b5c3f",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning\n",
    "\n",
    "We will use random search to find the best hyperparameters for our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7569002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# random forest hyperparameter tuning\n",
    "param_grid = {'n_estimators': [10, 100, 1000],\n",
    "              'max_depth': [None, 5, 10, 20],\n",
    "              'max_features': ['auto', 'sqrt'],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4]}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "rand_search = RandomizedSearchCV(rf, param_grid, cv=5, verbose=2, n_jobs=-1, n_iter=5)\n",
    "rand_search.fit(train_X, train_y)\n",
    "rand_search.best_params_, rand_search.best_score_, rand_search.best_estimator_.score(test_X, test_y)\n",
    "mse = mean_squared_error(test_X, pred_y)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "\n",
    "#grid_search = GridSearchCV(rf_model, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "#grid_search.best_estimator_ # best model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb4c43",
   "metadata": {},
   "source": [
    "## Save and load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb513f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best model\n",
    "\n",
    "joblib.dump(rand_search.best_estimator_, 'best_model.pkl')\n",
    "\n",
    "# load the best model\n",
    "best_model = joblib.load('best_model.pkl')\n",
    "best_model.score(test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301a11b",
   "metadata": {},
   "source": [
    "## Model interpretation\n",
    "\n",
    "scikit-learn provides multiple methods for model interpretation. Here we will see feature importance and permutation importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae9368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation\n",
    "# feature importance\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42, n_estimators=1000, max_depth=10, max_features='sqrt', min_samples_split=5, min_samples_leaf=2)\n",
    "rf.fit(train_X, train_y)\n",
    "rf.feature_importances_\n",
    "\n",
    "# plot feature importance\n",
    "importances = pd.Series(rf.feature_importances_, index=selected_columns)  #este selected columns vão ser as selecionadas na feature selection, mas como não está importado não lê esta variável\n",
    "importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25d2483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance based on permutation importance\n",
    "\n",
    "perm_importance = permutation_importance(rf, test_X, test_y)\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(selected_columns[sorted_idx[:10]], perm_importance.importances_mean[sorted_idx[:10]])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5cb8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pos_neg_idx = np.concatenate((sorted_idx[:10], sorted_idx[-10:]))\n",
    "plt.barh(selected_columns[pos_neg_idx], perm_importance.importances_mean[pos_neg_idx])\n",
    "plt.xlabel(\"Permutation Importance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
